{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples = 55809\n",
      "#test samples = 6201\n",
      "\n",
      "IOB tagging of the given sentence is::::::::::::::::::::::::\n",
      "[('The', 'DT', 'O'), ('United', 'NNP', 'B-org'), ('Nations', 'NNP', 'I-org'), ('has', 'VBZ', 'I-org'), ('administered', 'VBN', 'O'), ('Kosovo', 'NNP', 'B-tim'), ('since', 'IN', 'I-tim'), ('1999', 'CD', 'B-tim'), (',', ',', 'O'), ('since', 'IN', 'O'), ('NATO', 'NNP', 'B-org'), ('air', 'NN', 'O'), ('strikes', 'NNS', 'O'), ('against', 'IN', 'O'), ('Yugoslavia', 'NNP', 'B-gpe'), ('forced', 'VBD', 'I-gpe'), ('Yugoslav', 'NNP', 'B-tim'), ('and', 'CC', 'O'), ('Serbian', 'JJ', 'B-gpe'), ('security', 'NN', 'O'), ('forces', 'NNS', 'O'), ('from', 'IN', 'O'), ('the', 'DT', 'O'), ('area', 'NN', 'O')]\n",
      "\n",
      "Entity Recognition using IOB of the Given sentence is ::::::::::::::::::\n",
      "(S\n",
      "  The/DT\n",
      "  (org United/NNP Nations/NNP has/VBZ)\n",
      "  administered/VBN\n",
      "  (tim Kosovo/NNP since/IN)\n",
      "  (tim 1999/CD)\n",
      "  ,/,\n",
      "  since/IN\n",
      "  (org NATO/NNP)\n",
      "  air/NN\n",
      "  strikes/NNS\n",
      "  against/IN\n",
      "  (gpe Yugoslavia/NNP forced/VBD)\n",
      "  (tim Yugoslav/NNP)\n",
      "  and/CC\n",
      "  (gpe Serbian/JJ)\n",
      "  security/NN\n",
      "  forces/NNS\n",
      "  from/IN\n",
      "  the/DT\n",
      "  area/NN)\n",
      "\n",
      "Evaluating the accuracy of the IOB taggs is :  0.9185441941074524\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Aug  6 11:50:44 2017\n",
    "@ Main author: Abdul-Maalik\"\"\"\n",
    "\"@ editing & cahnging Junaid Ghauri\n",
    "\"\"\"\n",
    "********************************** Start DESCRIPTION *********************************************************************************************************\n",
    "\n",
    "STEP ONE: I have attach the english language corpus and python file and data_files folder must be in same place to run this program\n",
    "STEP TWO: I have created the NamedEntityRecognition class and define __init__ function.\n",
    "STEP THREE: I have set the corpus unzip correct path.\n",
    "STEP FOUR: I have read all files from corpus that have extension of .tags\n",
    "STEP FIVE: For every sentence in all files, every word is separated by 1 newline character. For every word, each annotation is separated by a tab character. \n",
    "STEP SIXTH: Then, IOB Tagging system contains tags of the form:\n",
    "    B-{CHUNK_TYPE} – for the word in the Beginning chunk\n",
    "    I-{CHUNK_TYPE} – for words Inside the chunk\n",
    "    O – Outside any chunk\n",
    "STEP SEVEN: I have extracted the features from .tags files\n",
    "STEP EIGHT: Then,I have defined conll_iob function that assign the IOB tags\n",
    "STEP NINE: I have created training sample and testing samples.\n",
    "STEP TEN: I have taken some datasets for training and then I have given input a sentence of english langauge \n",
    "STEP ELEVEN: input sentence is tokenized and then passed in a pos tagger built-in NLTK.\n",
    "STEP TWELTH: After that I have passed the input sentence to parse function and this function return a decision tree.   \n",
    "STEP THIRTEEN: Then I have passed decision tree to tree2conlltags built in funcion to get collocaion\n",
    "STEP FOURTEEN:After that I have display the result \n",
    "Step FIFTEEN: Finally,I have calculated the accuracy of my result\n",
    "STEP SIXTEEN : OUTPUT\n",
    "                        #training samples = 55809\n",
    "                        #test samples = 6102\n",
    "\n",
    "                        IOB tagging of the given sentence is::::::::::::::::::::::::\n",
    "                        [('The', 'DT', 'O'), ('United', 'NNP', 'B-org'), ('Nations', 'NNP', 'I-org'), ('has', 'VBZ', 'I-org'), ('administered', 'VBN', 'O'), ('Kosovo', 'NNP', 'B-tim'), ('since', 'IN', 'I-tim'), ('1999', 'CD', 'B-tim'), (',', ',', 'O'), ('since', 'IN', 'O'), ('NATO', 'NNP', 'B-org'), ('air', 'NN', 'O'), ('strikes', 'NNS', 'O'), ('against', 'IN', 'O'), ('Yugoslavia', 'NNP', 'B-gpe'), ('forced', 'VBD', 'I-gpe'), ('Yugoslav', 'NNP', 'B-tim'), ('and', 'CC', 'O'), ('Serbian', 'JJ', 'B-gpe'), ('security', 'NN', 'O'), ('forces', 'NNS', 'O'), ('from', 'IN', 'O'), ('the', 'DT', 'O'), ('area', 'NN', 'O'), ('.', '.', 'O')]\n",
    "\n",
    "                        Entity Recognition of the Given sentence is ::::::::::::::::::\n",
    "                        (S\n",
    "                          The/DT\n",
    "                          (org United/NNP Nations/NNP has/VBZ)\n",
    "                          administered/VBN\n",
    "                          (tim Kosovo/NNP since/IN)\n",
    "                          (tim 1999/CD)\n",
    "                          ,/,\n",
    "                          since/IN\n",
    "                          (org NATO/NNP)\n",
    "                          air/NN\n",
    "                          strikes/NNS\n",
    "                          against/IN\n",
    "                          (gpe Yugoslavia/NNP forced/VBD)\n",
    "                          (tim Yugoslav/NNP)\n",
    "                          and/CC\n",
    "                          (gpe Serbian/JJ)\n",
    "                          security/NN\n",
    "                          forces/NNS\n",
    "                          from/IN\n",
    "                          the/DT\n",
    "                          area/NN\n",
    "                          ./.)\n",
    "\n",
    "                        Evaluating the accuracy of the result is :  0.9156785243741765\n",
    "                \n",
    "STEP SEVENTEEN:In the above output:\n",
    "    First is length of training dataset and testing datasets\n",
    "    Second is Output of IOB function of input sentence like The is  \"O\", United is \"B-org\" and Nation is \"I-org\"\n",
    "    THird is Entity Recognition like United Nation and NATO is organization and Yugoslav is geo location means country \n",
    "    Fourth is evaluation of my results and output in line 55\n",
    "    \n",
    "*************************************************** END DESCRIPTION **************************************************\n",
    "\"\"\"\n",
    "\n",
    "import os,string\n",
    "import codecs\n",
    "import collections\n",
    "from collections import Iterable\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.chunk import conlltags2tree,tree2conlltags,ChunkParserI\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "\n",
    "ner_tags = collections.Counter() \n",
    "corpus_files = \"data\"\n",
    "\n",
    "class NamedEntityRecognition(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = NamedEntityRecognition.features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=NamedEntityRecognition.features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        \n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    "        return conlltags2tree(iob_triplets)\n",
    "    \n",
    "    def features(tokens, index, history):\n",
    "\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "        history = ['[START2]', '[START1]'] + list(history)\n",
    "\n",
    "        index += 2\n",
    "     \n",
    "        word, pos = tokens[index]\n",
    "        prevword, prevpos = tokens[index - 1]\n",
    "        prevprevword, prevprevpos = tokens[index - 2]\n",
    "        nextword, nextpos = tokens[index + 1]\n",
    "        nextnextword, nextnextpos = tokens[index + 2]\n",
    "        previob = history[index - 1]\n",
    "        contains_dash = '-' in word\n",
    "        contains_dot = '.' in word\n",
    "        allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    "     \n",
    "        allcaps = word == word.capitalize()\n",
    "        capitalized = word[0] in string.ascii_uppercase\n",
    "     \n",
    "        prevallcaps = prevword == prevword.capitalize()\n",
    "        prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    "     \n",
    "        nextallcaps = prevword == prevword.capitalize()\n",
    "        nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    "     \n",
    "        return {\n",
    "            'word': word,\n",
    "            'lemma': stemmer.stem(word),\n",
    "            'pos': pos,\n",
    "            'all-ascii': allascii,\n",
    "     \n",
    "            'next-word': nextword,\n",
    "            'next-lemma': stemmer.stem(nextword),\n",
    "            'next-pos': nextpos,\n",
    "     \n",
    "            'next-next-word': nextnextword,\n",
    "            'nextnextpos': nextnextpos,\n",
    "     \n",
    "            'prev-word': prevword,\n",
    "            'prev-lemma': stemmer.stem(prevword),\n",
    "            'prev-pos': prevpos,\n",
    "     \n",
    "            'prev-prev-word': prevprevword,\n",
    "            'prev-prev-pos': prevprevpos,\n",
    "     \n",
    "            'prev-iob': previob,\n",
    "     \n",
    "            'contains-dash': contains_dash,\n",
    "            'contains-dot': contains_dot,\n",
    "     \n",
    "            'all-caps': allcaps,\n",
    "            'capitalized': capitalized,\n",
    "     \n",
    "            'prev-all-caps': prevallcaps,\n",
    "            'prev-capitalized': prevcapitalized,\n",
    "     \n",
    "            'next-all-caps': nextallcaps,\n",
    "            'next-capitalized': nextcapitalized,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    def to_conll_iob(annotated_sentence):\n",
    "\n",
    "        global iob_tokens\n",
    "        iob_tokens = []\n",
    "        for idx, annotated_token in enumerate(annotated_sentence):\n",
    "            tag, word, ner = annotated_token\n",
    "         \n",
    "            if ner != 'O':\n",
    "                if idx == 0:\n",
    "                    ner = \"B-\" + ner\n",
    "                elif annotated_sentence[idx - 1][2] == ner:\n",
    "                    ner = \"I-\" + ner\n",
    "                else:\n",
    "                    ner = \"B-\" + ner\n",
    "            iob_tokens.append((tag, word, ner))\n",
    "        return iob_tokens\n",
    "                    \n",
    "    def read_tag_files(corpus_files):\n",
    "        \n",
    "        for root, dirs, files in os.walk(corpus_files):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".tags\"):\n",
    "                    \n",
    "                    with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                        file_content = file_handle.read().decode('utf-8').strip()\n",
    "                        annotated_sentences = file_content.split('\\n\\n')\n",
    "                        for annotated_sentence in annotated_sentences:\n",
    "                            annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    "     \n",
    "                            standard_form_tokens = []\n",
    "        \n",
    "                            for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                                annotations = annotated_token.split('\\t')\n",
    "                                word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    "                                            \n",
    "                                if ner != 'O':\n",
    "                                    ner = ner.split('-')[0]\n",
    "     \n",
    "                                if tag in ('LQU', 'RQU'):\n",
    "                                    tag = \"``\"\n",
    "     \n",
    "                                standard_form_tokens.append((word, tag, ner))\n",
    "        \n",
    "                            conll_tokens = NamedEntityRecognition.to_conll_iob(standard_form_tokens)\n",
    "                        \n",
    "                            yield [((w, t), iob) for w, t, iob in conll_tokens]\n",
    "\n",
    "\n",
    "    def Main():\n",
    "        \n",
    "        reader = NamedEntityRecognition.read_tag_files(corpus_files)\n",
    "        data = list(reader)\n",
    "        training_samples = data[:int(len(data) * 0.9)]\n",
    "        test_samples = data[int(len(data) * 0.9):]\n",
    "        \n",
    "        print (\"#training samples = %s\" % len(training_samples))\n",
    "        print (\"#test samples = %s\" % len(test_samples))\n",
    "        \n",
    "        print (\"\\nIOB tagging of the given sentence is::::::::::::::::::::::::\")\n",
    "        chunker = NamedEntityRecognition(training_samples[:500]) \n",
    "        sentance = \"The United Nations has administered Kosovo since 1999, since NATO air strikes against Yugoslavia forced Yugoslav and Serbian security forces from the area\" \n",
    "\n",
    "        tagged_sentance = pos_tag(word_tokenize(sentance))\n",
    "        NER_tree = chunker.parse (tagged_sentance)\n",
    "        iob_tagged = tree2conlltags(NER_tree)\n",
    "        print (iob_tagged)\n",
    "        \n",
    "        print (\"\\nEntity Recognition using IOB of the Given sentence is ::::::::::::::::::\")\n",
    "        print (NER_tree)\n",
    "        \n",
    "        score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs]) for iobs in test_samples[:50]])\n",
    "        print (\"\\nEvaluating the accuracy of the IOB taggs is :  \"+str(score.accuracy()))\n",
    "        \n",
    "        \n",
    "NamedEntityRecognition.Main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context free grammer Entity Recognition ::::::::::::::::::::::\n",
      "\n",
      "Started..................\n",
      "\n",
      "Named Phrase as a NP shown where appropriated ..................\n",
      "\n",
      "(S\n",
      "  The/DT\n",
      "  (NP (NBAR United/NNP Nations/NNP))\n",
      "  has/VBZ\n",
      "  administered/VBN\n",
      "  (NP (NBAR Kosovo/NNP))\n",
      "  since/IN\n",
      "  1999/CD\n",
      "  ,/,\n",
      "  since/IN\n",
      "  (NP (NBAR NATO/NNP air/NN strikes/NNS))\n",
      "  against/IN\n",
      "  (NP (NBAR Yugoslavia/NNP))\n",
      "  forced/VBD\n",
      "  (NP (NBAR Yugoslav/NNP))\n",
      "  and/CC\n",
      "  (NP (NBAR Serbian/JJ security/NN forces/NNS))\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (NP (NBAR area/NN))\n",
      "  ./.)\n",
      "\n",
      "Named Entity using context free grammer after matching with different list of named entities..................\n",
      "\n",
      "(S\n",
      "  The/DT\n",
      "  (org (NBAR United/NNP Nations/NNP))\n",
      "  has/VBZ\n",
      "  administered/VBN\n",
      "  (geo-loc (NBAR Kosovo/NNP))\n",
      "  since/IN\n",
      "  1999/CD\n",
      "  ,/,\n",
      "  since/IN\n",
      "  (org (NBAR NATO/NNP air/NN strikes/NNS))\n",
      "  against/IN\n",
      "  (geo-loc (NBAR Yugoslavia/NNP))\n",
      "  forced/VBD\n",
      "  (org (NBAR Yugoslav/NNP))\n",
      "  and/CC\n",
      "  (org (NBAR Serbian/JJ security/NN forces/NNS))\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (NP (NBAR area/NN))\n",
      "  ./.)\n",
      "\n",
      " Common Nouns are ['NATO air strikes', 'area']\n",
      "\n",
      " Named Phrases ['United Nations', 'Kosovo', 'NATO', 'Yugoslavia', 'Yugoslav', 'Serbian security forces']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "        from nltk import pos_tag, word_tokenize\n",
    "        import nltk\n",
    "        print (\"\\nContext free grammer Entity Recognition ::::::::::::::::::::::\")\n",
    "        print (\"\\nStarted..................\\n\")\n",
    "        ######################### Part of Speech tagging using nltk library #######################################\n",
    "        \n",
    "        sentance = \"The United Nations has administered Kosovo since 1999, since NATO air strikes against Yugoslavia forced Yugoslav and Serbian security forces from the area.\"\n",
    "        sent = \"The Turkey banned the Facebook due to waste of time\" \n",
    "        tagged_sentance = pos_tag(word_tokenize(sentance))\n",
    "       \n",
    "        ############################# List of Countries ans Organization ###########################################\n",
    "        \n",
    "        org = [\"United Nations\",\"UNESCO\",\"African Union\",\"UNICEF\",\"World Trade Organization\",\"Serbian security forces\",\"Google\",\"Microsoft\",\"Yugoslav\",\"Facebook\",\"NATO\"]\n",
    "        country = [\"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Turkey\",\"Uzbekistan\",\"Kosovo\",\"Venezuela\",\"Vietnam\",\"Virgin Islands\",\"Wallis and Futuna\",\"West Bank\",\"Yugoslavia\",\"Yemen\",\"Zambia\",\"Zimbabwe\"]\n",
    "        WeekDays = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "        ########################### Regular Expression are defined #################################################\n",
    "        \n",
    "        grammar = r\"\"\"\n",
    "            NBAR:\n",
    "                {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "\n",
    "            NP:\n",
    "                {<NBAR>}\n",
    "                {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "        \"\"\"\n",
    "        sentence_re = r'''(?x)      # set flag to allow verbose regexps\n",
    "              ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n",
    "            | \\w+(-\\w+)*            # words with optional internal hyphens\n",
    "            | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n",
    "            | \\.\\.\\.                # ellipsis\n",
    "            | [][.,;\"'?():-_`]      # these are separate tokens\n",
    "        '''\n",
    "        ################################# Apply Regular Expression on tagged sentance ##############################\n",
    "        \n",
    "        print (\"Named Phrase as a NP shown where appropriated ..................\\n\")\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        result = cp.parse(tagged_sentance)\n",
    "        print(result)\n",
    "        \n",
    "        ################################### Separate the Noun phrase from tagged data ##########################\n",
    "        \n",
    "        NER_NN  = list()\n",
    "        NER_NNP = list()\n",
    "        \n",
    "        for tag in result:\n",
    "            if isinstance (tag, nltk.tree.Tree):\n",
    "               \n",
    "                if tag.label()== 'NP':\n",
    "                    tag_list = tag.leaves()\n",
    "                    temp_NNP = \"\"\n",
    "                    temp_NN = \"\"\n",
    "                    \n",
    "                    for k in range (len(tag_list)): \n",
    "                        if 'NN' or 'NNS' in tag_list[k]:\n",
    "                            if temp_NN == \"\":\n",
    "                                temp_NN = tag_list[k][0]\n",
    "                            else:\n",
    "                                temp_NN = temp_NN+\" \"+tag_list[k][0]\n",
    "                            if tag_list[k][0] == \"time\":\n",
    "                                tag.set_label(\"tim\")\n",
    "\n",
    "                        if 'NNP' in tag_list[k]:\n",
    "                            if temp_NNP == \"\":\n",
    "                                temp_NNP = tag_list[k][0]\n",
    "                            else:\n",
    "                                temp_NNP = temp_NNP+\" \"+tag_list[k][0]\n",
    "                        if 'NNPS' in tag_list[k]:\n",
    "                            if temp_NNP == \"\":\n",
    "                                temp_NNP = tag_list[k][0]\n",
    "                            else:\n",
    "                                temp_NNP = temp_NNP+\" \"+tag_list[k][0]\n",
    "                                \n",
    "                        if 'JJ' in tag_list[k] and k == 0 and 'NN' in tag_list[k+1]:\n",
    "                            if tag_list[k+2]:\n",
    "                                temp_NNP= tag_list[k][0]+\" \"+tag_list[k+1][0]+\" \"+tag_list[k+2][0]\n",
    "                            else:\n",
    "                                temp_NNP= tag_list[k][0]+\" \"+tag_list[k+1][0]\n",
    "                                print (temp_NNP)\n",
    "                                    \n",
    "                                \n",
    "                    \n",
    "                    if temp_NN != \"\":\n",
    "                        if temp_NN != temp_NNP:\n",
    "                            NER_NN.append(temp_NN)\n",
    "                            \n",
    "        ####################### Noun phrase are match with different lists according to Data set like list of country and organization\n",
    "        \n",
    "                    if temp_NNP != \"\":\n",
    "                        for j in range(len(org)):\n",
    "                            if org[j] == temp_NNP:\n",
    "                                tag.set_label(\"org\")\n",
    "                                NER_NNP.append(temp_NNP)\n",
    "                        \n",
    "                        for k in range (len(country)):\n",
    "                            if country[k] == temp_NNP:\n",
    "                                tag.set_label(\"geo-loc\")\n",
    "                                NER_NNP.append(temp_NNP)\n",
    "                                \n",
    "        print (\"\\nNamed Entity using context free grammer after matching with different list of named entities..................\\n\")            \n",
    "        print (result)\n",
    "        \n",
    "        ############################ Noun phrase are printed #####################################\n",
    "        \n",
    "        print (\"\\n Common Nouns are \"+ str(NER_NN))\n",
    "        print (\"\\n Named Phrases \"+ str(NER_NNP)+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
